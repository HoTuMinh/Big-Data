# big data

### Big data 102

- overview
    
    > Big data has the ability to change the nature of a business
    > 
    - reality: MORE and MORE digital information
    - motivation behind the science of big data
        - data management
        - data modelling and analytics
        - visualization decisions and values to create products
        - tools
            
            ![image.png](image.png)
            
        - requirements
            - khả năng mở rộng
            - hiệu suất vào ra dữ liệu
            - khả năng chấp nhận lỗi
            - xử lý thời gian thực
            - hỗ trợ kích thước dữ liệu
            - hỗ trợ tác vụ lặp
        - what the heck is Big Data?
            - data that has great volume, fast velocity, great variety ⇒ needs special methods to handle (and optimize the handling)
            1. volume (growing at exponential rate, petabyte, using cloud to store)
            2. velocity (the volume increase really fast & handle data in real time - in ml sec)
            3. variety (>80% is non-structured data → blog, image, video …)
            4. veracity (độ tin cậy, really hard unfortunately:()
            5. value (**#1 question: what value does the data brings?)**
        - where does it come from (6 main sources)
            1. hành chính (từ tổ chức chính phủ or phi chính phủ) 
            2. thương mại (các giao dịch giữa hai thực thể) 
            3. các thiết bị cảm biến (vd khi chụp ảnh vệ tinh)
            4. thiết bị theo dõi (vd phone, GPS)
            5. hành vi (vd tìm kiếm trực tuyến, đọc các trang trực tuyến)
            6. thông tin về ý kiến, quan điểm của các cá nhân, tổ chức trên các phương diện thông tin xã hội  
- tools
    - Hadoop → framework cho phép sử dụng các distributed processing (ứng dụng phân tán) để quản lý và lưu trữ những tệp dữ liệu lớn
    - virtual machine → emulate a computer (can install different OS)
- part I
    - chap I. business motivation
        - definitions
            - **Big Data**
                - Big Data is a field dedicated to the analysis, processing, and storage of large collections of data that frequently originate from disparate source
                - a blend between mathematics, statistics, CS, and subject matter expertise
            - **datasets** = a collection or groups of related data
            - **data analysis** = the process of examining data to find facts, relationships, patterns, insights, trends
            - **data analytic** = the management of the complete data lifecycle (collecting → cleansing → organizing → storing → analyzing → governing)
                - big data analytic lifecycle
                    - identifying
                    - procuring
                    - preparing
                    - analyzing
            - **descriptive analytics** are carried out to answer questions about events that have already occurred
            - **diagnostic analytics** aim to determine the cause of a phenomenon that occurred in the past using questions that focus on the reason behind the event
            - **predictive analysis** are carried out in an attempt to determine the outcome of an event that might occur in the future
            - **prescriptive analytics** build upon the results of predictive analytics by prescribing actions that should be taken
            - **business intelligence (BI)** enables an organization to gain insight into the performance of an enterprise by analyzing data generated by it business processes and information systems
            - **key performance indicators (KPI)** is used to gauge success within a particular business context
                - to identify business performance problems
                - to demonstrate regulatory compliance
        - big data characteristics
            - must possess one or more
            1. volume
                
                ![image.png](image%201.png)
                
            2. velocity (arrives at fast speed)
                
                ![image.png](image%202.png)
                
            3. variety
                - multiple formats and the types of data
                - structured data
                - semi-structured data
                - unstructured data
                
                ![image.png](image%203.png)
                
            4. veracity 
                - the quality / fidelity of data
                - activities to resolve invalid data and remove noise
            5. value  
                - the usefulness of data for an enterprise
                
                ![image.png](image%204.png)
                
        - different types of data
            - structured / unstructured / semi-structured
            - machine-generated / human-generated
            - structured
                - conforms to a data model
                - often stored in a tabular form (most often in a relational database)
                - example:
                    - banking transactions
                    - invoices
                    - customer records
            - unstructured data
                - does not conform to a data model / schema
                - make up about 80% of the data within any given enterprise
                - has a faster grow rate than structured data
                - textual or binary
                - example
                    - images (binary)
                    - tweets (text)
                    - blog postings (text)
                - if it is required to be stored within a relational database, it is stored in a table as a *Binary Large Object (BLOB)*
                    - or *NoSQL* (non-relational database)
            - semi-structured data
                - has a defined level of structure and consistency, but is not relational in nature
                - hierarchical or graph-based
                - example
                    - XML
                    - JSON
                    - sensor data
                - often has special pre-processing and storage requirements
            - metadata
                - provides info about a dataset’s characteristics and structure
                - mostly machine-generated and can be appended to data
                - example
                    - XML tags
                    - attributes providing the file size of a digital photograph
        - case study
            - ETI: insurance company (health / building / marine / aviation)
            - the IT environment consists of a combination of client-server and mainframe platforms
            - 
    - chap II. technology motivation
        - chap 5
            - overview. Key concepts related to the storage of Big Data datasets. How it is different from relational database
            - abstract. Data acquired from external sources is often not in a format that can be directly processed → **data wrangling** (filter, cleanse, prepare data for downstream analysis).
                1. a copy of the data is 1st stored in its acquired format 
                2. the prepared data (after wrangling) needs to be stored again
                - when is storage required?
                    - external datasets are acquired, or internal data will be used in a Big Data environment
                    - data is manipulated to be made amendable for data analysis
                    - data is processed via a ETL activity, or output is generated as a result of an analytical operation
                
                *⇒ due to the need to store Big Data datasets, strategies and technologies have been created to achieve cost-effective and highly scalable storage solutions*
                
                - clusters
                    - is a tightly coupled collection of servers, or node (usually have the same hardware specs & are connected together via a network to work as a single unit)
                    - each node in the cluster has its own dedicated resources
                    - execute a task by splitting it into small pieces and distributing their execution onto different computers that belong to the cluster
                - file systems and distributed files systems
                    - A file is an atomic unit of storage used by the file system to store the data
                    - A DFS is a file system that can store large files spread across the nodes of a cluster
                - NoSQL
                    - not only SQL
                    - non-relational database
                    - designed to store semi-structured and unstructured data
                    - also support query languages other than SQL
                - sharding
                    - is the process of horizontally partioning a large dataset into a collection of smaller datasets called **shards**
                    - the shards are distributed across multiple nodes
                    - Sharding allows the distribution of processing loads to achieve **horizontal scalability**
                        - a method for increasing a syýtem’s capacity by adđing capacity resources
                    - (+) partial tolerance towards failure
                    - (-) queẻie requiring data from multiple shards will impose performance penalties
                - replication
                    - stores multiple copies of a dataset on multiple nodes
                    - 2 implementation methods
                        - master - slave
                            - all data is written to a maấter node
                            - once saved, the data is replicated over to multiple slave nodes
                            - all external write requests occur on the master node
                            - read requests can be fulfilled by any slave nodes
                            - better for reading than writing
                        - peer to peer
                            - each node is equally capable of handling read and write
                            - prone to wửite inconsistencies, address by
                                - Pessimistic concurrency → uses locking to ensure that only one update to a record can òccur at a time
                                - Optimistic concurrency → allow inconsistencies to occur with knowledge that eventually consistency will be achieved after all uodates have propagated
                            - To ensure reading consistency, a voting system can be implemented where a read is declared consistent if the majority of the peerế contain the same version of the record
                - CAP theorem
                - ACID
                - BASE
        - chap 6
        - chap 7
        - chap 8

 

### Hadoop 52

- what I will learn in the book Hadoop guide
    - learn fundamental components such as MapReduce, HDFS, and YARN
    - explore MapReduce in depth, including steps for developing applications with it
    - set up and maintain a Hadoop cluster running HDFS and MapReduce on YARN
    - learn 2 data formats: Avro for data serialization and Parquet for nested data
    - use data ingestion tools such as Flume (for streaming data) and Sqoop (for bulk data transfer)
    - understand how high-level data processing tools like Pig, Hive, Crunch, and Spark work with Hadoop
    - Learn the HBase distributed database and the ZooKeeper distributed configuration service
- Chapter I: Hadoop introduction
    - **data!**
        - nowadays, there are a crap load of data coming from
            - stock exchange
            - Facebook photos
            - genealogy site
            - the Internet Archive …
        - individuals’ digital streams are growing apace, as well as the machines’
        
        ⇒ big data is here, the bad news is that we are struggling to store and analyze it 
        
    - **data storage and analysis**
        - the problem: although the storage capacities of hard drives have increased massively over the years, access speeds have not kept up
        - one solution is to read from multiple disks at once
        - 2 problems
            1. hardware failure
                
                ⇒ RAID (keep copies of data)
                
                - Hadoop takes a different approach
            2. data needs to be combined 
                - MapReduce provides a programming model
    - **querying all your data**
        - MapReduce is a batch query processor
    - **Beyond Batch**
        - MapReduce is fundamentally a batch processing system
            
            → not suitable for interactive analysis 
            
            → best of offline use 
            
        - YARN is a cluster resource management system
        - processing patterns that work with Hadoop
            - interactive SQL
            - iterative processing
            - stream processing
            - search
        - Hadoop versus RDMS
            - On the other hand, for updating a small proportion of records in a database, a traditional
            B-Tree (the data structure used in relational databases, which is limited by the rate at
            which it can perform seeks) works well. For updating the majority of a database, a BTree is less efficient than MapReduce, which uses Sort/Merge to rebuild the database
            
            ![image.png](image%205.png)
            
            ![image.png](image%206.png)
            
        - Hadoop versus Grid Computing
        - Hadoop versus Volunteer Computing
    - **history of Apache Hadoop**
    - **what’s the book about?**
        
        I. core Hadoop 
        
        1. introduction to Hadoop 
        2. introduction to MapReduce
        3. Hadoop filesystems, HDFS
        4. YARN - Hadoop’s cluster resource management system
        5. I/O building blocks in Hadoop (data integrity, compression, serialization, file-based data structures)
        
        II. MapReduce in in depth 
        
        1. practical steps needed to develop a MapReduce application 
        2. how MapReduce is implemented in Hadoop, from the POV of a user 
        3. the MapReduce programming model 
        4. advanced MapReduce topics 
        
        III. the administration of Hadoop 
        
        10 & 11. how to set up and maintain a Hadoop cluster running HDFS and MapReduce on YARN 
        
        IV. projects that build on Hadoop 
        
        V. Case studies   
        
- Chapter II: MapReduce
    - is a programming model for data processing
    - weather dataset example
        - about the data
            - log data
            - stored using line-oriented ASCII format → each line is a record
            - data files are organized by date and weather station
            - *it is generally easier and more efficient to process a smaller number of relatively large files*
        - analyzing the data with Unix tools
            - [*awk](https://www.geeksforgeeks.org/awk-command-unixlinux-examples/)* → a classic tool for processing line-oriented data
            - *the complete run for the century took 42 mins in one run on a single EC2 High-CPU Extra Large instance*
            - speed up the processing process by running parts of the program in parallel
                1. different file sizes for different years → split the input into fized-size chunks 
                2. combining the results from independent processes may require further processing 
                3. limited by the processing capacity of a single machine 
        - analyzing the data with Hadoop
            - Map and Reduce
                - breaking the processing into 2 phases: map & reduce
                - each phase has key - value pairs as input and output (the type is chosen by the dev)
                - the dev also specifies 2 functions (well you already know :v)
            - detail
                - `map`
                    - input: raw NCDC data
                    - setting up the data for `reduce` to work
                    - pull out the year and air temp & drop bad records
                    - output: is processed by the MapReduce framework before being sent to `reduce`
                        - sorts and groups the key-value pairs by key
                        
                        ![image.png](image%207.png)
                        
                - `reduce`
                    - iterate thru the list and pick up the maximum reading
                    - output: the maximum global temperature recorded each year
                
                ![image.png](image%208.png)
                
    - Java MapReduce
        - 3 things
            1. map function
            2. reduce function
            3. some code to run the job 
        - Mapper
            
            ```java
            import java.io.IOException // handle input-output related errors
            import org.apache.hadoop.io.IntWritable // a Hadoop datatype that represents integer values (Hadoop uses its own data types to optimize data transfer across the network)
            import org.apache.hadoop.io.LongWritable // long integers values
            import org.apache.hadoop.io.Text // string 
            import org.apache.hadoop.mapreduce.Mapper // imports the Mapper class (base class for creating map functions in Hadoop). It allows you to define how the data is processesed in the Map phase 
            
            // defines the class, which extends the Mapper class 
            // it processes input key-value pairs (Long-Text) and outputs key-value pairts (Text(year)-Int(temparature))
            public class MaxTemperatureMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
            	private static final int MISSING = 9999;
            	
            	@Override
            	public void map(LongWritable key, Text value, Context context)
            		throws IOException, InterruptedException
            ```
            
        - Reducer
            
            ```java
            import java.io.IOException;
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Reducer;
            public class MaxTemperatureReducer
             extends Reducer<Text, IntWritable, Text, IntWritable> {
            
             @Override
             public void reduce(Text key, Iterable<IntWritable> values, Context context)
             throws IOException, InterruptedException {
            
             int maxValue = Integer.MIN_VALUE;
             for (IntWritable value : values) {
             maxValue = Math.max(maxValue, value.get());
             }
             context.write(key, new IntWritable(maxValue));
             }
            }
            ```
            
        - Job
- Appendix A
    - how to install Hadoop Common, HDFS, MapReduce, YARN
    - instructions are for Unix-based systems
    - Hadoop
        1. prerequisites
            - a suitable version of Java (check the [wiki](http://wiki.apache.org/hadoop/HadoopJavaVersions))
        2. download a stable release (gzipped tar file) 
        3. tell it where Java is located on your system 
            - convenient to create an environment variable that points to the Hadoop installation directory, and to put Hadoop binary directories on your command-line path
        4. check that Hadoop runs 
    - configuration
        - using XML files
        - located at `etc/hadoop` subdirectory
    - 3 modes to run
        - standalone (default properties)
            - no daemons running
            - suitable during development
            - easy to test and debug
        - pseudo distributed mode
            - run on the local machine
            - simulating a cluster on a small scale
            - steps
                - create a configuration file
                - configuring SSH to start daemons
                    1. make sure that SSH is installed 
                    2. enable password login 
                    3. test that you can connect with `localhost`
                - formatting the HDFS filesystem → run a command
                - starting and stopping the daemons
                - creating a user directory
        - fully distributed mode
        
        → set the appropriate properties & start the Hadoop daemons 
        

### Exercises

- 27.09
    - cài thành công Ubuntu
    - short cut
        - Host = right CTRL
        - Host + N = see performance
    - can turn off
    - can use a browser
    - can install applications
    - can adjust memory & performance allocation
- 28.09
    
    ![image.png](image%209.png)
    
    ![image.png](image%2010.png)
    
    ![image.png](image%2011.png)
    
    ![image.png](image%2012.png)
    
- 01.10
    - 01
        - [ ]  cài Java
        - [ ]  cài Hadoop
        - [ ]  HDFS
    - 02
        - [ ]  clone project về
        - [ ]  cài maven
- 02.10
    - [x]  cài đặt máy ảo ubuntu
        1. download Ubuntu ISO files 
        2. download Virtual Box
        3. use Virtual Box to boot up Ubuntu 
            1. set up the user 
            2. set up the memory 
    - [x]  cài đặt môi trường Python 3 (ubuntu)
        1. check if Python is installed = `python3`
    - [x]  cài đặt môi trường Java (ubuntu)
        1. check is Java is already installed = `java -version`
        2. if not → `sudo apt install default-jdk`
        3. verify the installation 
        
        ![image.png](image%2013.png)
        
        - JDK is a complete Java development and runtime environment → develop and run Java applications
        - JRE is a runtime environment only → just run Java applications
    - [x]  làm quen các lệnh shell script
        - list
            - getting information
            - monitoring performance and status
            - working with files navigating and working with directories
            - printing file and string contents
            - compression and archiving
            - performing network operations
            - bash shebang
            - pipes and filters
            - shell environment variables
            - metacharacters
            - quoting
            - I/O redirection
            - command substitution
            - command line arguments
            - batch vs. concurrent modes
            - scheduling jobs with Cron
    - [x]  [cài đặt Hadoop](https://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm)
        1. pre-installation set up → set up Linux using ssh
            1. creating a user 
            2. SSH setup and key generation 
                - to do different operations on a cluster such as starting, stopping, distributed daemon shell operations
                - To authenticate different users of Hadoop, it is required to provide public/private key pair for a Hadoop user and share it with different users
                - generating a key value pair using SSH. Copy the public keys form id_rsa.pub to authorized_keys, and provide the owner with read and write permissions to authorized_keys file respectively
                    
                    ```
                    $ ssh-keygen -t rsa 
                    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 
                    $ chmod 0600 ~/.ssh/authorized_keys 
                    ```
                    
            3. install Java 
        2. [download Hadoop](https://ultahost.com/knowledge-base/install-hadoop-on-ubuntu/) 
            1. tải file từ trên mạng về = `wget https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz`
            2. giải nén = `tar xzf hadoop-3.4.0.tar.gz`
            3. chuyển đường dẫn giải nén tới đường dẫn tải → để sắp xếp cho gọn (`/usr/local/hadoop`) = `sudo mv hadoop-3.4.0 /usr/local/hadoop`
            4. tìm đường tải Java = `readlink -f /usr/bin/java | sed "s:bin/java::"` 
            5. chỉnh file config môi trường Hadoop = using `nano` as the text editor (how [to use nano](https://ioflood.com/blog/nano-linux-command/#:~:text=To%20use%20the%20nano%20command,will%20create%20it%20for%20you))
                - `readlink -f /usr/bin/java | sed "s:bin/java::"`
    - [ ]  cài đặt HDFS
        - error
            - [1](https://stackoverflow.com/questions/48129029/hdfs-namenode-user-hdfs-datanode-user-hdfs-secondarynamenode-user-not-defined)
            - 2
    - [ ]  sử dụng lệnh với HDFS
    - [ ]  copy files into and out of the HDFS
    - [ ]  other
        - [x]  add hostname
            - a **hostname** is a user-generated custom name that identifies a computer system in a network
            - can be change using the Linux cmd
            - `hostname` → show host name
            - `sudo hostname 'new name'` → change the hostname without restarting (temporary)
            - `sudo hostnamectl set-hostname 'new name'` → after executing this command, the change will persist across reboots
                - may also need to edit the `/etc/hosts` file to reflect the new hostname to ensure all network services operate correctly with the new name
            - `hostnamectl` → view hostname
                - might have to install `systemd` → `sudo apt install systemd`
            - [edit the hosts file on ubuntu](https://pimylifeup.com/ubuntu-hosts-file/)
        - [x]  set environment
            1. verify that Java is installed = `java -version`
            2. get the location of JDK executable (Java Compiler)
                - `which java` → symbolic link (you have to follow it a couple of times), or
                - to get to the actual executable file → `readlink -f `which java` | sed “s:/bin/java::”`
            3. setting JAVA_HOME variable (temporarily)
                1. `export JAVA_HOME='path copied from the previous step'`
                2. check the value of `JAVA_HOME` directory 
            4. set the variable permanently 
                - add it to the bashrc file in the home directory
                1. back of your bashrc file = `cp ~/.bashrc ~/.bashrc.bak`
                2. use the echo command to append the export command = `echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/.bashrc`
                3. verify that it has been correctly added to the end of the file = `tail -3 ~/.bashrc`
                    
                    ![image.png](image%2014.png)
                    
            
            ! if you change the default Java version in the future, you’ll have to change the value of JAVA_HOME and point it to the correct executable path
            
        - [x]  add 1 normal user
            - after installing Ubuntu system, there is only an user you configured during installation (administrative user), except System Accounts
            - add a new user = `sudo adduser [name]`
            - [related](https://www.server-world.info/en/note?os=Ubuntu_24.04&p=initial_conf&f=1)
        - [x]  gen key
            - why?
                1. SSH authentication: when you connect to a remote server via SSH, using key-based authentication is more secure than using passwords
                2. GPG keys are used for encrypting emails, files, or signing data to ensure authenticity and privacy
                3. SSL/TLS certificates: When hosting a website, you may need to generate a key for an SSL/TLS certificate to secure the communication between your web server and clients (HTTPS)
            1. create key-pair by each user
                
                ```
                # create key-pair
                ubuntu@dlp:~$ ssh-keygen
                Generating public/private ed25519 key pair.
                Enter file in which to save the key (/home/ubuntu/.ssh/id_ed25519):   # Enter or input changes if you want
                Enter passphrase (empty for no passphrase):   # set passphrase (if set no passphrase, Enter with empty)
                Enter same passphrase again:
                Your identification has been saved in /home/ubuntu/.ssh/id_ed25519
                Your public key has been saved in /home/ubuntu/.ssh/id_ed25519.pub
                The key fingerprint is:
                SHA256:VPFjxEDeLKJLbKQurhgh0VIZxIrq8K+C+fg1iow8PmY ubuntu@dlp.srv.world
                The key's randomart image is:
                .....
                .....
                
                ubuntu@dlp:~$ ll ~/.ssh
                total 24
                drwx------ 2 ubuntu ubuntu 4096 Apr 26 06:47 ./
                drwxr-x--- 4 ubuntu ubuntu 4096 Apr 26 06:45 ../
                -rw------- 1 ubuntu ubuntu    0 Apr 26 01:09 authorized_keys
                -rw------- 1 ubuntu ubuntu  464 Apr 26 06:47 id_ed25519
                -rw-r--r-- 1 ubuntu ubuntu  102 Apr 26 06:47 id_ed25519.pub
                -rw------- 1 ubuntu ubuntu  978 Apr 26 06:28 known_hosts
                -rw-r--r-- 1 ubuntu ubuntu  142 Apr 26 06:28 known_hosts.old
                
                ubuntu@dlp:~$ cat ~/.ssh/id_ed25519.pub >> ~/.ssh/authorized_keys
                ```
                
            2. transfer the private key created on the server to a client, then it’s possible to login with key-pair authentication 
                
                ```
                ubuntu@node01:~$ mkdir ~/.ssh
                ubuntu@node01:~$ chmod 700 ~/.ssh
                # transfer the private key to the local ssh directory
                ubuntu@node01:~$ scp ubuntu@10.0.0.30:/home/ubuntu/.ssh/id_ed25519 ~/.ssh/
                ubuntu@10.0.0.30's password:
                id_rsa                                        100% 2655     1.8MB/s   00:00
                
                ubuntu@node01:~$ ssh ubuntu@10.0.0.30
                Enter passphrase for key '/home/ubuntu/.ssh/id_ed25519':   # passphrase if you set
                Welcome to Ubuntu 24.04 LTS (GNU/Linux 6.8.0-31-generic x86_64)
                
                .....
                .....
                
                ubuntu@dlp:~$     # logined
                ```
                
            3. If you set `[PasswordAuthentication no]`, it's more secure.
                
                ```
                root@dlp:~# vi /etc/ssh/sshd_config
                # line 57 : uncomment and change to [no]
                PasswordAuthentication no
                # line 62 : make sure the setting below
                KbdInteractiveAuthentication no
                # If the following file exists,
                # change the content in the same way or delete the file itself
                root@dlp:~# cat /etc/ssh/sshd_config.d/50-cloud-init.conf
                PasswordAuthentication yes
                root@dlp:~# rm /etc/ssh/sshd_config.d/50-cloud-init.conf
                root@dlp:~# systemctl restart ssh
                ```
                
            - [OpenSSH: SSH key-pair authentication](https://www.server-world.info/en/note?os=Ubuntu_24.04&p=ssh&f=4)
        - [x]  set namenode location
        - [ ]  set path for HDFS
            - modify the file `hdfs-site.xml` (a configuration file for the Hadoop Distributed File System - HDFS)
                
                → to set specific properties for how HDFS operates 
                
                ```
                <property>
                	<name>dfs.namenode.name.dir</name>
                	<value>/usr/local/hadoop/data/nameNode</value>
                	</property>
                ```
                
                - this property specifies the directory where the NameNode stores the file system’s metadata
                
                ```
                <property>
                	<name>dfs.datanode.data.dir</name>
                	<value>/usr/local/hadoop/data/datNode</value>
                	</property>
                ```
                
                - this property defines the directory where the DataNodes will store the actual data blocks of HDFS
                
                ```
                <property>
                	<name>dfs.replication</name>
                	<value>1</value>
                	</property>
                ```
                
                - this property sets the default replication factor for HDFS, which means how many copies of each data block should be stored across the cluster
        - [ ]  Local / Standalone mode
        - [ ]  [Pseudo Distributed Mode (Single Node Hadoop Deployment)](https://phoenixnap.com/kb/install-hadoop-ubuntu)
            - allows each Hadoop **daemons** to run as a single Java process
            1. `.bashrc`
            2. `hadoop-env.sh`
            3. `core-site.xml`
            4. `hdfs-site.xml`
            5. `mapred-site.xml`
            6. `yarn-site.xml`
- 04.10
    - lab 01 (continue)
        - [the link I follow](https://phoenixnap.com/kb/install-hadoop-ubuntu)
        - [`http://localhost:9870`](http://localhost:9870) → Hadoop NameNode UI
        - [`http://localhost:9864`](http://localhost:9864) → access individual DataNodes directly from the browser
        - [`http://localhost:8088`](http://localhost:8088) → the YARN resource manager
        - HDFS
            - **DFS (distributed file system)** → a concept of storing the file in multiple nodes in a distributed manner
            - HDFS stores the data in the form of block
            - Hadoop works on the MapReduce algorithm which is a master-slave architecture
                1. **NameNode(Master)**
                    - used for storing the metadata
                    - should have a high RAM
                    - receives heartbeat signals and block reports from all the slaves
                2. **DataNode(Slave)**
                    - mainly used for storing the data in a Hadoop cluster
        - [x]  [tương tác với HDFS](https://www.geeksforgeeks.org/hdfs-commands/)
    - lab 02
